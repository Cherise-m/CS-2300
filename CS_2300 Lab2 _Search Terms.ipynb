{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb0d657",
   "metadata": {},
   "source": [
    "# Search Terms #\n",
    "### done by Cherise Malisa ###\n",
    "\n",
    "In this lab data will be used to analyze the most searched for words on a website. The data first has to be refined inorder for the analysis to work. The data for this lab was provided by Direct-Supply, and it is a list of words and codes that were searched for and unit measure code(that was not used for this lab). The words not sorted or counted for frequency this is all done for the analysis of the code .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342689da",
   "metadata": {},
   "source": [
    "In the following cell the searchTerms file is opened and the  first column of data from the file is input into an array called search_phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1ad9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from spellchecker import SpellChecker\n",
    "search_phrases = []\n",
    "with open('searchTerms.csv', encoding = 'UTF-8') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        search_phrases.append(line.split(',')[0])\n",
    "# print(search_phrases[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea6e3a",
   "metadata": {},
   "source": [
    "This method splits each phrase given from the previously created array list, into seperate words and places those into a list. For example if we had the words chicken breast, as one word, in the new list we have chicken on its own and breast on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "979e68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliiting_phrases(search_phrases):\n",
    "    \n",
    "    tokens = []\n",
    "    for x in search_phrases:\n",
    "        if \" \" in x:\n",
    "            tokens += x.split(' ')\n",
    "        else:\n",
    "            tokens.append(x)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0375e8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 459 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_phrases = spliiting_phrases(search_phrases)\n",
    "\n",
    "# print(split_phrases[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba856b",
   "metadata": {},
   "source": [
    "This method removes any %20 otherwise known as web-spaces. Then the proper word is put into a list of cleaned tokens. For example if a word, name%20tags will then be put into cleaned tokens as nametags. by removing the %20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42324778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_tokens(tokens):\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for x in tokens:\n",
    "        if '%20' in x:\n",
    "            clean_tokens.append(x.replace('%20',' '))\n",
    "            \n",
    "        else:\n",
    "            clean_tokens.append(x)\n",
    "                \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa1ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 421 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleaned_tokens = cleaning_tokens(split_phrases)\n",
    "\n",
    "# print(cleaned_tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257e03",
   "metadata": {},
   "source": [
    "getting rid of numbers and punctuation marks and spaces code inspired by:\n",
    "studytonight.com/python-howtos/remove-numbers-from-string-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e315b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def removing_numbers(cleaned_tokens):\n",
    "    \n",
    "    just_words = []\n",
    "    listx = []\n",
    "    \n",
    "    for x in cleaned_tokens:\n",
    "            translation = str.maketrans('','',string.digits)\n",
    "            translation2 = str.maketrans('','',string.punctuation)\n",
    "            word = x.translate(translation)\n",
    "            word2 = word.translate(translation2)\n",
    "            listx.append(word2)\n",
    "        \n",
    "    for i in listx:\n",
    "         if not i == '':\n",
    "            just_words.append(i)\n",
    "            \n",
    "    return just_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acf9980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CMED', 'KEND', 'CMED', 'DYNCH', 'DYND', 'DEES', 'KC', 'LINK', 'PC', 'KEND', 'SA', 'KEND', 'bacon', 'pineapple', 'enfit', 'cheese', 'cheddar', 'buttermilk', 'milk', 'chicken', 'breast', 'DRIT', 'romain', 'banana', 'sl', 'UROCH', 'HUDS', 'bacon', 'name tags', 'cut', 'fruit', 'HUDS', 'milk', 'biscuit', 'way', 'milk', 'GERI', 'HCS', 'bacon', 'mash', 'liquid', 'egg', 'wipes', 'DYNDH', 'bacon', 'creamer', 'HUDS', 'lettuce', 'apple', 'sauce', 'cottage', 'milk', 'lactose', 'milk', 'JUICE', 'ORG', 'milk', 'biscuit', 'creamer', 'bottled', 'water', 'pearls', 'banana', 'dinner', 'rolls', 'sausage', 'link', 'ct', 'oil', 'WHITE', 'BREAD', 'green', 'beans', 'bacon', 'kits', 'avocado', 'Bacon', 'banana', 'honeydew', 'sausage', 'link', 'bacon', 'corn', 'flakes', 'bacon', 'bacon', 'banana', 'bacon', 'DRIT', 'coffee', 'bacon', 'bacon', 'panko', 'bacon', 'banana', 'magic', 'cup', 'sausage', 'link', 'msc']\n"
     ]
    }
   ],
   "source": [
    "just_words = removing_numbers(cleaned_tokens)\n",
    "print(just_words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dbc64",
   "metadata": {},
   "source": [
    "This method takes the list of cleaned up tokens and counts how many times they occur in the file of search terms. The pair of word and the times it occurs is then placed into a dictionay,where the key is the word and the value is number of times the occurrence. The most frequent statements seem to be the ones where the key is a number, and then the mixed number ones are the second most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5172c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def occurrences(just_words):\n",
    "    \n",
    "    term_frequency = {}\n",
    "\n",
    "    occurrence = 0\n",
    "    \n",
    "#     for x in range(len(just_words)):\n",
    "#         word = just_words[x]\n",
    "#         for i in just_words:\n",
    "#             if word == i:\n",
    "#                 occurrence += 1\n",
    "#         just_words.remove(word)\n",
    "#         term_frequency[word] = occurrence\n",
    "\n",
    "    unique_words = set(just_words)\n",
    "    \n",
    "    for words in unique_words:\n",
    "        term_frequency[words] = just_words.count(words)\n",
    "    \n",
    "#     term_frequency = Counter(just_words)\n",
    "  \n",
    "    return term_frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c0b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('InterDry', 1), ('Mar', 9), ('shorbread cookies', 1), ('condiment', 2), ('pea pearl onions', 1), ('KLEEN', 1), ('pizza', 2269), ('PROTECTIVE', 4), ('le', 4), ('coffeemate', 8)]\n"
     ]
    }
   ],
   "source": [
    "term_frequcny_dict = occurrences(just_words)\n",
    "# print(term_frequcny_dict)\n",
    "# dict_items = term_frequcny_dict.items()\n",
    "# first_hundred = list(dict_items)[:10]\n",
    "# print(first_hundred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6f066",
   "metadata": {},
   "source": [
    "The most frequent tokens were mostly food items, meat, canned good, fresh products and most of them were words correctly spelt.\n",
    "\n",
    "There are also some 'words' in that dictionary that are completely numerical. This could be because there are some people, from the company itself e.g storage manager or customer service, who are using the serach tool and know the codes for the supplies theyre seacrhing for. This could be the case with some of the words like for example,'CR-2344' which could be a code for one of the prodcuts. Some could just be misspelt words or words that were not-completed during entry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacea09a",
   "metadata": {},
   "source": [
    "The following method goes through a set of tokens that have been cleaned twice(1st from removing the %20 then from insuring no non-words are in the considered), and checks the splelling of each word. The correct word, if the word was mispelt, will then be generated using spellchecker. This spellchecker will take in the misplelt word and finds the potential correctly spelt word that the user would have maybe inteneded to put. Then these are loaded into a dictionary with the misspelled word/term as the key and the correct-version of that word as the value.\n",
    "code inspired by: https://pypi.org/project/pyspellchecker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0df363f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(distance = 1)\n",
    "\n",
    "def checking_spelling(just_words):\n",
    "    \n",
    "\n",
    "    spell_checking_dictionary = {}\n",
    "    corrected_list = []\n",
    "\n",
    "    just_words_lower = []\n",
    "    for x in just_words:\n",
    "    \n",
    "        just_words_lower.append(x.lower())\n",
    "    print(just_words_lower[0:100])\n",
    "\n",
    "    misspelled_words_token = []\n",
    "    misspelled_words_token = spell.unknown(just_words_lower)\n",
    "#     print(\"next list coming up:\")\n",
    "#     print(misspelled_words_token)\n",
    "\n",
    "    for word in just_words_lower:\n",
    "        if word in misspelled_words_token:\n",
    "            correct_word = spell.correction(word)\n",
    "            corrected_list.append(correct_word)\n",
    "            spell_checking_dictionary[word] = correct_word\n",
    "\n",
    "        else:\n",
    "            spell_checking_dictionary[word] = word\n",
    "            corrected_list.append(correct_word)\n",
    "            \n",
    "    return spell_checking_dictionary, corrected_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "750895d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cmed', 'kend', 'cmed', 'dynch', 'dynd', 'dees', 'kc', 'link', 'pc', 'kend', 'sa', 'kend', 'bacon', 'pineapple', 'enfit', 'cheese', 'cheddar', 'buttermilk', 'milk', 'chicken', 'breast', 'drit', 'romain', 'banana', 'sl', 'uroch', 'huds', 'bacon', 'name tags', 'cut', 'fruit', 'huds', 'milk', 'biscuit', 'way', 'milk', 'geri', 'hcs', 'bacon', 'mash', 'liquid', 'egg', 'wipes', 'dyndh', 'bacon', 'creamer', 'huds', 'lettuce', 'apple', 'sauce', 'cottage', 'milk', 'lactose', 'milk', 'juice', 'org', 'milk', 'biscuit', 'creamer', 'bottled', 'water', 'pearls', 'banana', 'dinner', 'rolls', 'sausage', 'link', 'ct', 'oil', 'white', 'bread', 'green', 'beans', 'bacon', 'kits', 'avocado', 'bacon', 'banana', 'honeydew', 'sausage', 'link', 'bacon', 'corn', 'flakes', 'bacon', 'bacon', 'banana', 'bacon', 'drit', 'coffee', 'bacon', 'bacon', 'panko', 'bacon', 'banana', 'magic', 'cup', 'sausage', 'link', 'msc']\n",
      "['med', 'kind', 'med', 'lynch', 'dyed', 'dyed', 'ke', 'ke', 'pa', 'kind', 'kind', 'kind', 'kind', 'kind', 'unfit', 'unfit', 'unfit', 'unfit', 'unfit', 'unfit', 'unfit', 'dirt', 'dirt', 'dirt', 'so', 'urdoch', 'hugs', 'hugs', 'nametags', 'nametags', 'nametags', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'his', 'his', 'his', 'his', 'his', 'his', 'dyndh', 'dyndh', 'dyndh', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'it', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'dirt', 'mic', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'wow', 'mic', 'mic', 'mic', 'mic', 'mic', 'mic', 'mic', 'mic', 'mic', 'fitplusxlg', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'chicken base', 'i', 'i', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'lactaid', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'pic', 'mic', 'mic', 'mic', 'mic', 'mic', 'pic', 'pic', 'fitplusxl', 'fitplusxl', 'fitplusxl']\n"
     ]
    }
   ],
   "source": [
    "correctly_spelled_dict, list_of_correct = checking_spelling(just_words)\n",
    "# dictionary_with_correctwords = occurrences(list_of_correct)\n",
    "# print(dictionary_with_correctwords)\n",
    "print(list_of_correct[0:200])\n",
    "# dict_items = correctly_spelled_dict.items()\n",
    "# first_hundred = list(dict_items)[:100]\n",
    "# print(first_hundred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7c7c5",
   "metadata": {},
   "source": [
    "The following cell stores the dictionary of clean words into a spreadsheet of two colums with the correct word and its number of occurences. \n",
    "Code inspired from:https://www.kite.com/python/answers/how-to-write-a-dictionary-to-a-.csv-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f429bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17204/2032170398.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary_with_correctwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moccurrences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_correct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary_with_correctwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_of_correct' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e57182da",
   "metadata": {},
   "source": [
    "The method below takes in the dictionary of correclty spelt words and their occurences and places those in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b29d8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# def creating_file():\n",
    "    \n",
    "a_file = open(\"count2.csv\",\"w\")\n",
    "writer = csv.writer(a_file)\n",
    "for key,value in dictionary_with_correctwords.items():\n",
    "    writer.writerow([key,value])\n",
    "    \n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a27c54",
   "metadata": {},
   "source": [
    "The results for the first dictionary with the mispelt words I think is more accurate because most of that words were misspelled and the corrected ones may not be what the user had intended but rather the word correclty spelled closer to the misspelled word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a88c",
   "metadata": {},
   "source": [
    "Slowest Cell analysis: The slowest cell is the occurrences method calculation. I think it is the slowest because it has to take each word in the already large list, the go through the whole list looking for words that are the same and then add up the occurences and put those in a list. so because it has to search the list for each word and it is a long list it runs for a very long time.\n",
    "\n",
    "Big-O analysis = n^2 \n",
    "Estimation of time of slowest cell: 1hr at most "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
